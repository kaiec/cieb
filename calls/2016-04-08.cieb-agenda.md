LD4PE CompIndex Editorial Board - Telecon 9 (Fri 2016-04-08)

* Agenda:   https://github.com/ld4pe/cieb/blob/master/calls/2016-04-08.cieb-agenda.md
* Previous: https://github.com/ld4pe/cieb/blob/master/calls/2016-03-11.cieb-notes.md
* Expected: Tom, Sean, Stuart, Debbie. Regrets: Magnus, Kai.
* Time:     0600 SFO / 0900 NYC / 1500 BER
* WebEx:    https://meetings.webex.com/collabs/meetings/join?uuid=MDFKPRG5QD43HA2LKEUXLBGN3L-JV0D
  * Meeting number: 196 227 489 (meeting does not require a password)
  * Audio Connection +1-415-655-0001 US TOLL
  * Access code: 196 227 489
  * Support: https://meetings.webex.com/collabs/#/support

  CIEB Call - Friday, 8 May 2016
Fri, Apr 8, 6:00 am | 1 hr
San Francisco (Pacific Daylight Time, GMT-07:00)
Host: Stuart Sutton

When it's time, join the meeting from here:
https://meetings.webex.com/collabs/meetings/join?uuid=M35W1OR0MAMVGXZ89PSPRN8HJH-JV0D

Agenda
I know that Magnus and Kai will not be joining but adding them to the roster so a recording will be available to them.

Access Information
Where: WebEx Online
Meeting number: 190 952 037
Meeting password: This meeting does not require a password.

Audio Connection
+1-415-655-0001 US TOLL
Access code: 190 952 037


https://github.com/ld4pe/cieb/blob/master/calls/2016-03-11.resource_types_jan2016.pdf 

----------------------------------------------------------------------
CIEB calls April thru June - Fridays at the usual time (see above)

* http://doodle.com/poll/np47qhu7tc6k46q6

    March 11
    April 1
    April 22 
    May 13 
    June 3 
    June 24    
    ...then one or two final calls in September/October

----------------------------------------------------------------------
Public review of Competency Index

-- https://docs.google.com/document/d/1i1k5kZLWncb3dJozb61SQiRS7hljUAox-gEXs4gHs7o

----------------------------------------------------------------------
Gaps in the Competency Index - and what to do about them

Stuart will explain that ASN will not publish a competency index until it is 
considered "complete" (or complete enough).  Please study the current CI for 
a discussion of which parts _must_ be filled in for the index to be complete 
enough -- and which parts we might consider dropping for the final release.

See Competency Index (above) plus:
-- Sean's "1-to-5" ratings of how well specific topics are covered in the index
   https://github.com/ld4pe/cieb/blob/master/calls/2016-03-11.resource_representation_jan2016.pdf

----------------------------------------------------------------------
Contacting "prolific" authors

On the February 19 call, we discussed the idea of writing to the some of the
authors of resources indexed by LD4PE, saying (roughly) "we indexed your video
X using the competencies Y and Z" and asking them to comment on the Competency
Index.

Goal: decide on process for approaching some authors, with a follow-up 
schedule.

-- Sean's lists of potential candidates for being approached
   https://github.com/ld4pe/cieb/blob/master/calls/2016-03-11.resource_creators.docx
   https://github.com/ld4pe/cieb/blob/master/calls/2016-03-11.resource_creators_v2.docx
-- Scratch Pad for notes on gaps, people to contact
   https://github.com/ld4pe/cieb/blob/master/calls/ScratchPad.md

----------------------------------------------------------------------
Scope Notes for the Topic Index

-- https://github.com/ld4pe/cieb/blob/master/calls/2016-03-11.TopicVocab-definitions.txt
   On the call, we will look at each heading for which Sean raised a 
   question and decide on what a scope note should say (to be wordsmithed
   offline).

----------------------------------------------------------------------
Instructions for Using the RDF Cataloging Tool (Sean)

-- See the tool at 
   http://explore.dublincore.net/rdf/lrmi/#/resource

-- Sean's "Instructions" draft
   https://docs.google.com/document/d/1KKDP6QZSLcfKdVhB3kOvUFqdbPq3k1WNDb_pITC0bTg/edit?usp=sharing
   https://github.com/ld4pe/cieb/blob/master/calls/2016-03-11.cataloging_tool_instructions.docx

======================================================================
Links

-  LD4PE Project homepage
   http://explore.dublincore.org/
-  Competency Index as published 
   http://explore.dublincore.net/comp-index/
-  CIEB scratchpad (gaps, people to contact)
   https://github.com/ld4pe/cieb/blob/master/calls/ScratchPad.md

LD4PE CompIndex Editorial Board - Telecon 8 (Fri 2016-03-11) - notes

* Notes:    https://github.com/ld4pe/cieb/blob/master/calls/2016-03-11.cieb-notes.md
* Previous: https://github.com/ld4pe/cieb/blob/master/calls/2016-02-19.cieb-notes.md
* Attended: Tom, Sean, Kai, Stuart, Debbie. Regrets: Magnus.

----------------------------------------------------------------------
CIEB calls April thru June - Fridays at the usual time (see above)

* http://doodle.com/poll/np47qhu7tc6k46q6

    March 11
    April 1
    April 22 
    May 13 
    June 3 
    June 24    
    ...then one or two final calls in September/October

Debbie and Stuart will add their availability.

----------------------------------------------------------------------
Public review of Competency Index

-- https://docs.google.com/document/d/1i1k5kZLWncb3dJozb61SQiRS7hljUAox-gEXs4gHs7o

__Stuart__: Status: We need to finalize what is here for the next 
version.  Hoping we have no more than two major updates between 
here and June.

----------------------------------------------------------------------
Gaps in the Competency Index - and what to do about them

Stuart will explain that ASN will not publish a competency index until it is 
considered "complete" (or complete enough).  Please study the current CI for 
a discussion of which parts _must_ be filled in for the index to be complete 
enough -- and which parts we might consider dropping for the final release.

See Competency Index (above) plus:
-- Sean's "1-to-5" ratings of how well specific topics are covered in the index
   https://github.com/ld4pe/cieb/blob/master/calls/2016-03-11.resource_representation_jan2016.pdf

__Stuart__: Two issues:
* where do we not have competencies (at all)?
* where are there gaps in the resources?

As of 2016-03-The following topics in the CI have no competencies defined:

### Fundamentals of Linked Data
#### Web technology

Write competency along the lines:
* HTML (needed for viewing and for RDFa), HTTP, REST, URI schemes

#### Linked Data principles

Write competencies along the lines:
* Five Stars -- one competency for each star?

#### Linked Data architectures and services

__Stuart__: If we look at Sean's report reflects how many resources 
have been indexed to each topic.

__ACTION__: Sean to say what the (low - 2.0) number of resources tagged
with this topic are actually about.

#### Non-RDF Linked Data

__Stuart__: When we were discussing the scope of the grant, Marjie Hlava was
adamant that RDF was not the only way to do linked data.

__Debbie__: Microdata?

__Stuart__: Schema.org makes no claims about Linked Data.  Lots of literals.
There are lots of people who want to use it that way, but.

__ACTION__: Tom to ask Marjie to elaborate on what she meant - suggest 
some competencies.

### RDF vocabularies and application profiles
#### Mapping

Write competencies along the lines:
* SKOS mapping properties
* Development of mapping predicates
* owl:sameAs, as well as controversy surrounding its use

### Creating and transforming Linked Data
#### Managing identifiers (URI)

Write competencies along the lines:
* Namespace policies
* Redirect services such as purl.org, w3id.org

__Stuart__: When I look at the subject indexing... Sean has 
fifteen resources.

__ACTION__: Sean to say what these resources are about (and 
thus how he interpreted the topic).

__ACTION__: Tom to ask John Kunze to propose a competency 
related to management of ARKs.

#### Versioning RDF data

__Tom__: Tempted to remove.

__Stuart__: Shouldn't remove things just because we have no competencies.
Mechanisms for handling versioning and updating, like Github.

Write competencies along the lines:
* Periodic numbered releases versus more incremental approaches
  that continually evolve (Github)...?

#### RDF data provenance

__Tom__: Propose to drop.

__Kai__: Strong relationship between versioning and provenance,
but cannot easily formulate competencies.  Under RDF Vocabularies - 
who created it?  Approach taken: always "depends".  If you ask 
me for core competence - "differentiate simple approach and 
representing the chain".

__ACTION__: Kai to write competency about "differentiate simple approach and
representing the chain".

### Interacting with RDF data
#### Programming RDF data

__ACTION__: Tom to propose RDF.rb competencies to run by Tom Johnson

__ACTION__: Stuart to write to Diane.

A Fundamentals of Resource Description Framework 
*  Identity in RDF 
  * Knows that anything can be named with URIs, such as agents, places, events, artifacts, and concepts
*  RDF data model 
  * Knows the subject-predicate-object component structure of a triple 
  * Understands that URIs and literals denote things in the world ("resources") real, imagined, or conceptual
  * Understands the difference between literals and non-literal resources
  * Understands that resources are declared members (instances) of classes using the property rdf:type
  * Understands the use of datatypes and language tags with literals
  * Understands blank nodes and their uses
  * Understands that QNames define shorthand prefixes for long URIs 
    * Uses prefixes for URIs in RDF specifications and data
  * Understands the concept of the named graph
  * Articulates differences between the RDF abstract data model and the XML and relational models
  * Understands the use of RDF Schema to create and interpret RDF vocabularies
  * Understands the RDF abstract data model as a directed labeled graph
  * Knows graphic conventions for depicting RDF-based models
    * Can use graphing or modeling software to share those models with others
*  Related data models
  * Grasps essential differences between schemas for syntactic validation (e.g., XML) and for inferencing (RDF Schema)
  * Differentiates hierarchical document models (eg, XML) and graph models (RDF)
*  RDF serialization
  * Distinguishes the RDF abstract data model and concrete serializations of RDF data
    * Expresses RDF data in Turtle syntax
    * Expresses RDF data in RDFa syntax
  * Understands RDF serializations as interchangeable encodings of a given set of triples (RDF graph)
    * Uses tools to convert RDF data between different serializations
A Fundamentals of Linked Data
*  Web technology
*  Linked data principles
*  Linked Data architectures and services
*  Linked Data policies and best practices
  * Knows the primary organizations related to Linked Data standardization
    * Participates in developing standards and best practice with relevant organizations such as W3C
*  Non-RDF Linked Data
A RDF vocabularies and application profiles
*  Finding RDF-based vocabularies
  * Retrieves and accesses RDF data from the "open Web"
*  Designing RDF-based vocabularies
  * Uses RDF Schema to express semantic relationships within a vocabulary
    * Correctly uses sub-class relationships in support of inference
    * Correctly uses sub-property relationships in support of inference
  * Knows the naming conventions for RDF properties and classes
  * Reuses published properties and classes where available
    * Uses portals and registries to find existing RDF-based vocabularies
  * Coins namespace URIs, as needed, for any new properties and classes required
    * Drafts a policy for coining URIs for properties and classes
    * Chooses "hash"- or "slash"-based URI patterns based on requirements
*  Maintaining RDF vocabularies
  * Understands policy options for persistence guarantees
    * Can draft a persistence policy
*  Versioning RDF vocabularies
  * Knows technical options for the form, content, and granularity of versions
    * Can express and justify a versioning policy
*  Publishing RDF vocabularies
  * Understands the typical publication formats for RDF vocabularies and their relative advantages
  * Understands the purpose of publishing RDF vocabularies in multiple formats using content negotiation
*  Mapping RDF vocabularies
*  RDF application profiles
  * Identifies real-world entities in an application domain as requirements for RDF classes
  * Identifies resource attributes and relationships between domain entities as requirements for RDF properties
  * Investigates how others have modeled the same or similar application domains
    * Communicates a domain model with words and diagrams
    * Participates in the social process of developing application profiles
A Creating and transforming RDF Data
*  Managing identifiers (URIs)
*  Creating RDF data
  * Structures data using blank nodes where appropriate 
  * Generates RDF data from non-RDF sources
*  Versioning RDF data
*  RDF data provenance
*  Cleaning and reconciling RDF data
  * Cleans a dataset by finding and correcting errors, removing duplicates and unwanted data
*  Mapping and enriching RDF data
  * Uses available resources for named entity recognition, extraction, and reconciliation
A Interacting with RDF Data
*  Finding RDF Data
  * Retrieves and accesses RDF data from the "open Web"
  * Uses relevant resources to discover existing Linked Data datasets
  * Monitors and updates lists which report the status of SPARQL endpoints
  * Uses available vocabularies for dataset description to support their discovery
  * Registers datasets with relevant services for discovery   
*  Programming RDF Data
*  Querying RDF Data
  * Understands that a SPARQL query matches an RDF graph against a pattern of triples with fixed and variable values
  * Understands the basic syntax of a SPARQL query
    * Uses angle brackets for delimiting URIs
    * Uses question marks for indicating variables
    * Uses PREFIX for base URIs
  * Formulates advanced queries on data containing blank nodes
  * Demonstrates a working knowledge of the forms and uses of SPARQL result sets (SELECT, CONSTRUCT, DESCRIBE, and ASK)
    * Uses the SELECT clause to identify the variables to appear in a table of query results
    * Uses the WHERE clause provide the graph pattern to match against the graph data
    * Uses variables in SELECT and WHERE clauses to yield a table of results
    * Uses ASK for a True/False test for a match to a query pattern
    * Uses DESCRIBE to extract a single graph containing RDF data about resources
    * Uses CONSTRUCT to extract and transform results into a single RDF graph specified by a graph template
  * Understands how to combine and filter graph patterns using operators such as UNION, OPTIONAL, FILTER, and MINUS
    * Uses UNION to formulate queries with multiple possible graph patterns
    * Uses OPTIONAL to formulate queries to return the values of optional variables when available
    * Uses FILTER to formulates queries that eliminate solutions from a result set
    * Uses NOT EXISTS to limit whether a given graph pattern exists in the data
    * Uses MINUS to remove matches from a result based on the evaluation of two patterns
    * Uses NOT IN to restrict a variable to not being in a given set of values
  * Understands the major SPARQL result set modifiers, e.g., to limit or sort results, or to return distinct results only once
    * Uses ORDER BY to define ordering conditions by variable, function call, or expression
    * Uses DISTINCT to ensure solutions in the sequence are unique
    * Uses OFFSET to control where the solutions processed start in the overall sequence of solutions
    * Uses LIMIT to restrict the number of solutions processed for query results
    * Uses projection to transforms a solution sequence into one involving only a subset of the variables 
  * Understands the use of SPARQL functions and operators
    * Uses the regular expression (regex()) function for string matching
    * Uses aggregates to apply expressions over groups of solutions (GROUP BY, COUNT, SUM, AVG, MIN) for partitioning results, evaluating projections, and filtering
    * Uses the lang() function to return the language tag of an RDF literal
    * Uses the langMatches() function to match a language tag against a language range
    * Uses the xsd:decimal(expn) function to convert an expression to an integer
    * Uses the GROUP BY clause to transforms a result set so that only one row will appear for each unique set of grouping variables 
    * Uses the HAVING clause to apply a filter to the result set after grouping    
  * Differentiates between a Default Graph and a Named Graph, and formulates queries using the GRAPH clause
    * Formulates advanced queries using FROM NAMED and GRAPH on local data
    * Formulate advanced queries using FROM NAMED on remote data
  * Formulate advanced queries using subqueries
  * Uses a temporary variable to extend a query
  * Understands the role of Property Paths and how they are formed by combining predicates with regular expression-like operators
  * Understands the concept of Federated Search
    * Formulates advanced queries on a remote SPARQL endpoint using the SERVICE directive
    * Uses federated query to query over a local graph store and one or more other SPARQL endpoints
    * Pulls data from a different SPARQL endpoints in one single query using the SERVICE directive
  * Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by a third party tools and APIs
  * Formulates queries using FROM with URLs and local files
  * Reads and understands high-level descriptions of the classes and properties of a dataset in order to write queries 
  * Uses available tools, servers, and endpoints to issue queries against a dataset 
    * Execute SPARQL queries using the Jena ARQ command-line utility
    * Queries multiple local data files using ARQ
    * Uses ARQ to evaluate queries on local data
    * Uses Fuseki server to evaluate queries on a dataset
    * Queries multiple data files using Fuseki
    * Accesses DBPedia's SNORQL/SPARQL endpoint and issues simple queries     
*  Visualizing RDF Data
  * Uses publicly available tools to visualize data
    * Uses Google FusionTables to create maps and charts
  * Distills results taken from large datasets so that visualizations are human-friendly
  * Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by a third party tools and APIs 
*  Reasoning over RDF data
  * Understands the principles and practice of inferencing
    * Uses common entailment regimes and understands their uses
  * Understands the role of formally declared domains and ranges for inferencing
  * Understands how reasoning can be used for integrating diverse datasets
*  Assessing RDF data quality
*  RDF Data analytics
  * Uses available ontology browsing tools to explore the ontologies used in a particular dataset 
*  Manipulating RDF Data
  * Knows the SPARQL 1.1 Update language for updating, creating, and removing RDF graphs in a Graph Store
    * Uses INSERT/DELETE to update triples
    * Uses a CONSTRUCT query to preview changes before executing an INSERT/DELETE operation
  * Knows the SPARQL 1.1 Graph Store HTTP protocol for updating graphs on a Web server (in "RESTful" style)
    * Uses GET to retrieve triples from a default graph or a named graph
    * Uses PUT to insert set of triples into a new graph (or replace an existing graph)
    * Uses DELETE to remove a graph
    * Uses POST to add triples to an existing graph
    * Uses proper syntax to request specific media types, such as Turtle
  * Understands the difference between SQL query language (which operates on database tables) and SPARQL (which operates on RDF graphs)
A Creating Linked Data applications
*  Storing RDF data
*  Linked Data application architecture
*  Linked Data mashups

PROPOSALS
* Sean proposes - for "Web technology", "Programming RDF data", or "LD Application architecture":

  * Understands that RDFa is a way to express RDF data within HTML, in a way
    that is machine-readable, and by reusing the existing human-readable data
    in the document.
    * Uses RDFa to publish Linked Data in HTML, XHTML, XML, and SVG documents.
    * Uses appropriate tools to edit, visualize, and debug HTML+RDFa code. 
  * Understands that markups such as RDFa can be used to generate better search
    listings and improve visibility on the Web.
    * Uses RDFa to express Rich Snippets or schema.org, so that web pages will be
      displayed in an enhanced format on all major search engines.
    * Uses Open Graph Protocol to express concepts that are contained in web
      pages (e.g., people, places, events, movies and recipes). 
  * Employs JSON-LD in order to use Linked Data in Web-based programming
    environments, to build interoperable Web services, and to store Linked Data
    in JSON-based storage engines.

* Sean proposes -- for "Linked Data architectures and services":
  * Understands Linked Data query processing is a special case of federated
    query processing with special challenges caused by the highly distributed
    structure and evolving nature of Linked Data.
    * Generates candidate execution plans and selects one of these plans for
      actual execution.
    * Considers criteria including: minimizing the overall execution time,
      minimizing response time, minimizing network traffic, or maximizing the
      degree of result completeness
  * Understands that a URI is relevant for a given query if looking up this URI
    gives us data that contributes to the query result.
    * Given a Linked Data query, determines a set of URIs to look up (i.e.,
      source selection) which works towards selecting all relevant URIs and only
      relevant URIs
    * Knows that advantages and disadvantages of both index-based and live
      exploration strategies for source selection.
  * Understands that there are integrated and separated approaches to query
    execution.
    * Knows that integrated execution combines the data retrieval and result
      construction process into one phase.
  * Knows that separated execution approaches clearly separate data retrieval
    from result construction by two consecutive phases.

* Sean proposes, for "Designing RDF-based vocabularies":
  * Understands that OWL (Web Ontology Language) is a more expressive option
    for data modeling and reasoning than RDFS.
    * Understands that OWL allows one to describe data in terms of set operations
      (e.g. unionOf).
    * Understands that OWL allows one to define equivalences across datasets
      (e.g. sameAs).
    * Understands that OWL allows one to restrict property values (e.g.
      allValesFrom).
    * Uses OWL annotations (e.g., owl:versionInfo, owl:backwardsCompatibleWith,
      owl:deprecatedProperty) to express relationships between different
      ontologies. 
  * Understands that Simple Knowledge Organization System (SKOS) is a simpler
    alternative to RDFS and OWL suitable for tasks such as describing
    controlled vocabularies.
  * Uses classes to describe concepts (loosely defined as “ideas”), concept
    collections, and concept schemes.  
    * Uses properties (e.g., skos:exactMatch, skos: broader, and skos:narrower)
      to map between separate vocabularies.  [Sean: put under "Mapping RDF vocabularies"?]

* Sean proposes, under "Managing identifiers":
  * Understands the principle of persistence, i.e. that a URI is permanently
    assigned to a particular resource, is stable, and does not change or vanish
    over time.
    * Avoids including version numbers and status information in URIs
    * Exercises caution when using auto-increment for minting new URIs
    * Avoids using query strings in URIs
    * Avoids using organization names in URIs
    * Avoids using file extensions in URIs
  * Understands the principle of dereferencability, i.e. a user agent can make
    a request to a URI over the Internet and receive a meaningful response
    back.
    * Ensures that if the user agent is a Web browser, then what comes back is a
      human readable HTML document
    * Ensures that If the user agent is an RDF client then RDF is returned from
      the same URI.
  * Understands that URIs should be unambiguous, meaning that there should be
    no confusion between identifiers for Web documents and identifiers for
    other resources
    * Implements 303 URIs for real-world resources to avoid ambiguity.
  * In order to create and manage URIs, one should be the owner of the
    respective Internet domain and/or have administrator’s rights on it
    * Uses a dedicated service independent of the data originator (e.g. Dublin
      Core and purl.org)
    * Is aware of the existence of any national URI policies or dedicated
      services for persistence.

* Sean proposes, under "Mapping and enriching RDF data":
  * Is aware that the W3C has defined a direct mapping from relational
    representation to an RDF representation of data.
    * Uses the direct mapping to materialize RDF graphs or define virtual graphs,
      which can be queried by SPARQL or traversed by an RDF graph API.
  * Is aware of the companion specification, the R2RML mapping language, that
    allows the creation of customized mapping from relational data to RDF.
    * Uses a default mapping generated by R2RML for further customization.

* Sean proposes, under "Finding RDF Data" => "...discover datasets":
  * Downloads whole datasets provided as one big file (i.e., a "data dump")

* Sean proposes, under "Programming RDF Data":
  * Understands that working with very large datasets requires distributed
    storage (e.g., Hadoop) and processing (MapReduce). [Sean: Should this be 
    under "Storing RDF data"?]
  * Uses a programming language (e.g., Javascript) to request data from a
    Linked Data service.
    * Use the jQuery function “ajax” to call a query against an endpoint. 
  * Understands that the RDFLib library defines a plugin interface for parsers,
    stores, and serializers that other packages can use.
    * Imports available common namespaces (e.g., RDFS, OWL, XSD, SKOS) directly
      from rdflib.
    * Uses appropriate packages (e.g sparql-client, SPARQLWrapper) for SPARQL
      implementation with RDFLib.
  * Understands that the primary interface RDFLib exposes to work with RDF is a
    “Graph”.
  * Selects the appropriate type of “Graph” from the RDFLib graph Module:
    Graph, Conjunctive Graph, Quoted Graph, and Dataset.
    * Knows that the class Graphs supports the python “in” operator, as well as
      iteration and some operations like union, difference and intersection.
    * Uses the class ConjunctiveGraph to form an aggregation of all the named
      graphs in a store.
    * Uses the class Quoted Graph to implement N3 formulae for scenarios such as
      implication and other such processing.

* Sean proposes, under "Reasoning over RDF data":
  * Understands that the different “flavors” of OWL trade off expressive
    modeling power for computational efficiency when performing reasoning.
    * Knows that previous versions of OWL (OWL/Lite, OWL/DL, and OWL/Full)
      mentioned in older documents have been superseded by newer OWL 2 flavors.
    * Uses OWL 2/Full to take advantage of all available constructs when
      reasoning performance is not a concern.
    * Uses OWL 2/EL when there are a large number of classes and properties
      linked together through complicated relationships and an automated reasoner
      will be used to draw out further relationships.
    * Uses OWL 2/QL for querying large amounts of instance data that sit in
      traditional relational databases.
    * Uses OWL 2/RL for applications which are built on a business rules engine
      using complicated class relationships. 

* Sean proposes, under "Storing RDF data":
  * Understands that database systems designed for the storage and retrieval of
    RDF data  are called “triple stores”
    * Knows that some popular RDF stores include Sesame, Jena, Redland, and
      OpenLink Virtuoso.
  * Understands that data can be copied into a centralized repository to create
    a data warehouse which can then be queried.

GAPS
* Mr DC (Wellywood Staplegun) suggested a competency to understand where LD
  fits in a broader context -- e.g., the difference between LD and the Internet
  of Things, or between a relational DB and a triplestore.

ACTIONS
-- Tom to respond Mike Lauruhn: We do not think that it is appropriate to classify 
   competencies by proficiency level (Beginner, etc).  However, we _do_ think it is 
   appropriate to classify particular resources by level.
